{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Scrapper \ud83e\uddf9","text":"<p>If you were looking for a web scraper that actually works, you just found it. It\u2019s called Scrapper! Downloading a page from the Internet and then extracting an article from it in a structured format is not as easy as it may seem at first glance. Sometimes it can be damn hard. But Scrapper will help you with this, I hope :) To do this, Scrapper has plenty of features.</p> <p>Scrapper is a free and open-source product. It combines the power and experience of related open-source projects, so give it a try first.</p>"},{"location":"#quick-start","title":"Quick start","text":"<p>Quick start a scrapper instance:</p> <pre><code>docker run -d -p 3000:3000 --name scrapper amerkurev/scrapper:latest\n</code></pre> <p>Scrapper will be available at http://localhost:3000/. For more details, see Usage</p>"},{"location":"#demo","title":"Demo","text":"<p>Watch a 30-second demo reel showcasing the web interface of Scrapper.</p>"},{"location":"#features","title":"Features","text":"<p>The main features of Scrapper are:</p> <ul> <li>The headless browser is already built-in. Modern websites actively use JavaScript on their pages, fight against crawlers, and sometimes require user actions, such as agreeing to the use of cookies on the site. All of these tasks are solved by the Scrapper using the excellent Playwright project. The Scrapper container image is based on the Playwright image, which already includes all modern browsers.</li> <li>The Read mode in the browser is used for parsing. Millions of people use the \u201cRead\u201d mode in the browser to display only the text of the article on the screen while hiding the other elements of the page. Scrapper follows the same path by using the excellent Readability.js library from Mozilla. The parsing result will be the same as in the Read mode of your favorite browser.</li> <li>A simple and beautiful web interface. Working with Scrapper is easy and enjoyable because you can do it right in your browser. The simple web interface allows you to debug your query, experiment with each parameter in the API, and see the result in HTML, JSON, or a screenshot. The beautiful design helps achieve an excellent Pico project. A dark theme for comfortable reading is also available.</li> <li>The Scrapper REST API is incredibly simple to use as it only requires a single call and just a few parameters making it easy to integrate into any project. Furthermore, the web interface offers a visual query-builder that simplifies the learning process for the user.</li> <li>Scrapper can search for news links on the main pages of websites. This is difficult because there may be not only links to news but also links to other parts of the website. However, Scrapper can distinguish between them and select only links to news articles.</li> </ul> <p>And many other features:</p> <ul> <li>Stealth mode. Various methods are used to make it difficult for websites to detect a Headless browser and bypass web scraping protection.</li> <li>Caching results. All parsing results are saved to disk or S3-compatible object storage, and you can access them later by API without repeating the whole request.</li> <li>Page screenshots. Headless browsers don\u2019t have a window, but screenshots allow you to see the page as it appears to the parser. This is very useful!</li> <li>Incognito mode or persistent sessions. You can configure the browser to work in incognito mode or without it. In this case, the browser will save session data such as cookies and local storage to disk. To use them again.</li> <li>Proxy support. HTTP/SOCKS4/SOCKS5 proxy work is supported.</li> <li>Fully customizable. You can control a lot through the API: additional HTTP headers, viewport for device emulation, Readability parser settings, and much more.</li> <li>Delivered as a Docker image. Scrapper is built and delivered as a Docker image, which is very easy to deploy for testing and production. No dependencies or installations on the host. All you need to run Scrapper is Docker.</li> <li>Free license. Scrapper doesn\u2019t ask for money, insert ads, or track your actions ever. And if you want to help the project develop further, just give us a star on GitHub \u2b50</li> </ul>"},{"location":"#supported-architectures","title":"Supported architectures","text":"<ul> <li>linux/amd64</li> <li>linux/arm64</li> </ul>"},{"location":"#status","title":"Status","text":"<p>The project is under active development and may have breaking changes till <code>v1</code> is released.  However, we are trying our best not to break things unless there is a good reason. As of version <code>v0.8.0</code>, Scrapper is considered good enough for real-life usage, and many setups are running it in production.</p>"},{"location":"#license","title":"License","text":"<p>Apache-2.0 license</p>"},{"location":"sections/api/","title":"API Reference","text":""},{"location":"sections/api/#get-apiarticleurl","title":"GET /api/article?url=\u2026","text":"<p>The Scrapper API is very simple. Essentially, it is just one call that can easily be demonstrated using the cURL:</p> <pre><code>curl -X GET \"localhost:3000/api/article?url=https://en.wikipedia.org/wiki/web_scraping\"\n</code></pre> <p>Use the GET method on the <code>/api/article</code> endpoint, passing one required parameter <code>url</code>. This is the full URL of the webpage on the Internet that contains an article. Scrapper will load the webpage in a browser, extract the article text, and return it in JSON format in the response.</p> <p>All other request parameters are optional and have default values. However, you can customize them to your liking. The table below lists all the parameters that you can use, along with their descriptions and default values. To make it easier to build requests, use the web interface where the final request link is generated in real-time as you configure the parameters.</p>"},{"location":"sections/api/#request-parameters","title":"Request Parameters","text":""},{"location":"sections/api/#scrapper-settings","title":"Scrapper settings","text":"Parameter Description Default <code>url</code> Page URL. The page should contain the text of the article that needs to be extracted. <code>cache</code> All results of the parsing process will be cached in the <code>user_data</code> directory. Cache can be disabled by setting the cache option to false. In this case, the page will be fetched and parsed every time. Cache is enabled by default. <code>true</code> <code>full-content</code> If this option is set to true, the result will have the full HTML contents of the page (<code>fullContent</code> field in the response). <code>false</code> <code>stealth</code> Stealth mode allows you to bypass anti-scraping techniques. It is disabled by default. <code>false</code> <code>screenshot</code> If this option is set to true, the result will have the link to the screenshot of the page (<code>screenshot</code> field in the response). Important implementation details: Initially, Scrapper attempts to take a screenshot of the entire scrollable page. If it fails because the image is too large, it will only capture the currently visible viewport. <code>false</code> <code>user-scripts</code> To use your JavaScript scripts on a webpage, put your script files into the <code>user_scripts</code> directory. Then, list the scripts you need in the <code>user-scripts</code> parameter, separating them with commas. These scripts will run after the page loads but before the article parser starts. This means you can use these scripts to do things like remove ad blocks or automatically click the cookie acceptance button. Keep in mind, script names cannot include commas, as they are used for separation.For example, you might pass <code>remove-ads.js, click-cookie-accept-button.js</code>.If you plan to run asynchronous long-running scripts, check <code>user-scripts-timeout</code> parameter. <code>user-scripts-timeout</code> Waits for the given timeout in milliseconds after users scripts injection. For example if you want to navigate through page to specific content, set a longer period (higher value). The default value is 0, which means no sleep. <code>0</code>"},{"location":"sections/api/#browser-settings","title":"Browser settings","text":"Parameter Description Default <code>incognito</code> Allows creating <code>incognito</code> browser contexts. Incognito browser contexts don\u2019t write any browsing data to disk. <code>true</code> <code>timeout</code> Maximum operation time to navigate to the page in milliseconds; defaults to 60000 (60 seconds). Pass 0 to disable the timeout. <code>60000</code> <code>wait-until</code> When to consider navigation succeeded, defaults to <code>domcontentloaded</code>. Events can be either:<code>load</code> - consider operation to be finished when the <code>load</code> event is fired.<code>domcontentloaded</code> - consider operation to be finished when the DOMContentLoaded event is fired.<code>networkidle</code> - consider operation to be finished when there are no network connections for at least 500 ms.<code>commit</code> - consider operation to be finished when network response is received and the document started loading. <code>domcontentloaded</code> <code>sleep</code> Waits for the given timeout in milliseconds before parsing the article, and after the page has loaded. In many cases, a sleep timeout is not necessary. However, for some websites, it can be quite useful. Other waiting mechanisms, such as waiting for selector visibility, are not currently supported. The default value is 0, which means no sleep. <code>0</code> <code>resource</code> List of resource types allowed to be loaded on the page. All other resources will not be allowed, and their network requests will be aborted. By default, all resource types are allowed. The following resource types are supported: <code>document</code>, <code>stylesheet</code>, <code>image</code>, <code>media</code>, <code>font</code>, <code>script</code>, <code>texttrack</code>, <code>xhr</code>, <code>fetch</code>, <code>eventsource</code>, <code>websocket</code>, <code>manifest</code>, <code>other</code>. Example: <code>document,stylesheet,fetch</code>. <code>viewport-width</code> The viewport width in pixels. It\u2019s better to use the <code>device</code> parameter instead of specifying it explicitly. <code>viewport-height</code> The viewport height in pixels. It\u2019s better to use the <code>device</code> parameter instead of specifying it explicitly. <code>screen-width</code> The page width in pixels. Emulates consistent window screen size available inside web page via window.screen. Is only used when the viewport is set. <code>screen-height</code> The page height in pixels. <code>device</code> Simulates browser behavior for a specific device, such as user agent, screen size, viewport, and whether it has touch enabled.Individual parameters like <code>user-agent</code>, <code>viewport-width</code>, and <code>viewport-height</code> can also be used; in such cases, they will override the <code>device</code> settings.List of available devices. <code>iPhone 12</code> <code>scroll-down</code> Scroll down the page by a specified number of pixels. This is particularly useful when dealing with lazy-loading pages (pages that are loaded only as you scroll down). This parameter is used in conjunction with the <code>sleep</code> parameter. Make sure to set a positive value for the <code>sleep</code> parameter, otherwise, the scroll function won\u2019t work. <code>0</code> <code>ignore-https-errors</code> Whether to ignore HTTPS errors when sending network requests. The default setting is to ignore HTTPS errors. <code>true</code> <code>user-agent</code> Specific user agent. It\u2019s better to use the <code>device</code> parameter instead of specifying it explicitly. <code>locale</code> Specify user locale, for example en-GB, de-DE, etc. Locale will affect navigator.language value, Accept-Language request header value as well as number and date formatting rules. <code>timezone</code> Changes the timezone of the context. See ICU\u2019s metaZones.txt for a list of supported timezone IDs. <code>http-credentials</code> Credentials for HTTP authentication (string containing username and password separated by a colon, e.g. <code>username:password</code>). <code>extra-http-headers</code> Contains additional HTTP headers to be sent with every request. Example: <code>X-API-Key:123456;X-Auth-Token:abcdef</code>."},{"location":"sections/api/#network-proxy-settings","title":"Network proxy settings","text":"Parameter Description Default <code>proxy-server</code> Proxy to be used for all requests. HTTP and SOCKS proxies are supported, for example http://myproxy.com:3128 or socks5://myproxy.com:3128. Short form myproxy.com:3128 is considered an HTTP proxy. <code>proxy-bypass</code> Optional comma-separated domains to bypass proxy, for example <code>.com, chromium.org, .domain.com</code>. <code>proxy-username</code> Optional username to use if HTTP proxy requires authentication. <code>proxy-password</code> Optional password to use if HTTP proxy requires authentication."},{"location":"sections/api/#readability-settings","title":"Readability settings","text":"Parameter Description Default <code>max-elems-to-parse</code> The maximum number of elements to parse. The default value is 0, which means no limit. 0 <code>nb-top-candidates</code> The number of top candidates to consider when analysing how tight the competition is among candidates. 5 <code>char-threshold</code> The number of characters an article must have in order to return a result. 500"},{"location":"sections/api/#response-fields","title":"Response fields","text":"<p>The response to the <code>/api/article</code> request returns a JSON object that contains fields, which are described in the table below.</p> Parameter Description Type <code>byline</code> author metadata null or str <code>content</code> HTML string of processed article content null or str <code>dir</code> content direction null or str <code>excerpt</code> article description, or short excerpt from the content null or str <code>fullContent</code> full HTML contents of the page null or str <code>id</code> unique result ID str <code>url</code> page URL after redirects, may not match the query URL str <code>domain</code> page\u2019s registered domain str <code>lang</code> content language null or str <code>length</code> length of extracted article, in characters null or int <code>date</code> date of extracted article in ISO 8601 format str <code>query</code> request parameters object <code>meta</code> social meta tags (open graph, twitter) object <code>resultUri</code> URL of the current result, the data here is always taken from cache str <code>screenshotUri</code> URL of the screenshot of the page null or str <code>siteName</code> name of the site null or str <code>textContent</code> text content of the article, with all the HTML tags removed null or str <code>title</code> article title null or str <code>publishedTime</code> article publication time null or str"},{"location":"sections/api/#error-handling","title":"Error handling","text":"<p>If an error (or multiple errors) occurs during the execution of a request, the response structure will be as follows:</p> <pre><code>{\n  \"detail\": [\n    {\n      \"type\": \"error_type\",\n      \"msg\": \"some message\"\n    }\n  ]\n}\n</code></pre> <p>Some errors do not have a detailed description in the response to the request. In this case, you should refer to the log of the Docker container to investigate the cause of the error.</p>"},{"location":"sections/api/#get-apilinksurl","title":"GET /api/links?url=\u2026","text":"<p>To collect links to news articles on the main pages of websites, use a different query on the <code>/api/links</code> endpoint. The query parameters are similar, but the Readability settings are not required for this query because no text is extracted. Instead, the Link parser is used, which has its own set of parameters. A description of these parameters is provided below.</p> <pre><code>curl -X GET \"localhost:3000/api/links?url=https://www.cnet.com/\"\n</code></pre>"},{"location":"sections/api/#request-parameters_1","title":"Request Parameters","text":""},{"location":"sections/api/#link-parser-settings","title":"Link parser settings","text":"Parameter Description Default <code>text-len-threshold</code> The median (middle value) of the link text length in characters. The default value is 40 characters. Hyperlinks must adhere to this criterion to be included in the results. However, this criterion is not a strict threshold value, and some links may ignore it. 40 <code>words-threshold</code> The median (middle value) of the number of words in the link text. The default value is 3 words. Hyperlinks must adhere to this criterion to be included in the results. However, this criterion is not a strict threshold value, and some links may ignore it. 3"},{"location":"sections/api/#response-fields_1","title":"Response fields","text":"<p>The response to the <code>/api/links</code> request returns a JSON object that contains fields, which are described in the table below.</p> Parameter Description Type <code>fullContent</code> full HTML contents of the page str <code>id</code> unique result ID str <code>url</code> page URL after redirects, may not match the query URL str <code>domain</code> page\u2019s registered domain str <code>date</code> date when the links were collected in ISO 8601 format str <code>query</code> request parameters object <code>meta</code> social meta tags (open graph, twitter) object <code>resultUri</code> URL of the current result, the data here is always taken from cache str <code>screenshotUri</code> URL of the screenshot of the page str <code>links</code> list of collected links list <code>title</code> page title str"},{"location":"sections/auth/","title":"HTTPS and Authentication","text":"<p>Enhance the security of your Scrapper deployment with HTTPS and Basic Authentication by integrating Caddy server.</p> <p>This approach is recommended for instances exposed to the internet and can be configured with minimal effort using Docker Compose.</p>"},{"location":"sections/auth/#configuring-caddy-for-security","title":"Configuring Caddy for Security","text":"<p>Caddy handles SSL certificate issuance and renewal through Let\u2019s Encrypt and supports Basic Authentication for added security.</p> <p>To configure Caddy with Scrapper:</p>"},{"location":"sections/auth/#1-customize-the-caddyfile","title":"1. Customize the Caddyfile","text":"<p>Update <code>scrapper.localhost</code> to your domain name. For Basic Authentication, generate a secure hashed password with <code>caddy hash-password</code> and update the Caddyfile with this hash. To generate a password hash:</p> <pre><code>caddy hash-password -plaintext 'your_new_password'\n</code></pre> <p>Replace <code>your_new_password</code> with a strong password, then insert the hashed result into the Caddyfile.</p>"},{"location":"sections/auth/#2-launch-with-docker-compose","title":"2. Launch with Docker Compose","text":"<p>With your <code>docker-compose.yml</code> and edited Caddyfile ready, deploy the services:</p> <pre><code>docker compose up -d\n</code></pre>"},{"location":"sections/auth/#secure-access-to-scrapper","title":"Secure Access to Scrapper","text":"<p>Once deployed, access Scrapper at <code>https://your_domain</code>. You\u2019ll be asked for the username and password specified in the Caddyfile.</p>"},{"location":"sections/auth/#automatic-certificate-renewal","title":"Automatic Certificate Renewal","text":"<p>Caddy automatically renews SSL certificates before they expire, requiring no action from the user. Enjoy uninterrupted HTTPS protection for your Scrapper instance without manual intervention.</p>"},{"location":"sections/usage/","title":"Usage","text":""},{"location":"sections/usage/#getting-scrapper","title":"Getting Scrapper","text":"<p>The Scrapper Docker image is based on the Playwright image, which includes all the dependencies needed to run a browser in Docker and also includes the browsers themselves. As a result, the image size is quite large, around 2 GB. Make sure you have enough free disk space, especially if you plan to take and store screenshots frequently.</p> <p>To get the latest version of Scrapper, run:</p> <pre><code>docker pull amerkurev/scrapper:latest\n</code></pre>"},{"location":"sections/usage/#creating-directories","title":"Creating directories","text":"<p>Scrapper uses two directories on the disk. The first one is the <code>user_data</code> directory. This directory contains browser session data such as cookies and local storage. Additionally, the cache of Scrapper\u2019s own results (including screenshots) is stored in this directory.</p> <p>The second directory is <code>user_scripts</code>. In this directory, you can place your own JavaScript scripts, which you can then embed on pages through the Scrapper API. For example, to remove ads blocks or click the \u201cAccept Cookies\u201d button (see the <code>user-scripts</code> parameter in the API Reference section for more information).</p> <p>Scrapper does not work from the root user inside the container. Instead, it uses a user with UID <code>1001</code>. Since you will be mounting the <code>user_data</code> and <code>user_scripts</code> directories from the host using Bind Mount, you will need to set write permissions for UID <code>1001</code> on these directories on the host. </p> <p>Here is an example of how to do this:</p> <pre><code>mkdir -p user_data user_scripts\n\nchown 1001:1001 user_data/ user_scripts/\n\nls -l\n</code></pre> <p>The last command (<code>ls -l</code>) should output a result similar to this:</p> <pre><code>drwxr-xr-x 2 1001 1001 4096 Mar 17 23:23 user_data\ndrwxr-xr-x 2 1001 1001 4096 Mar 17 23:23 user_scripts\n</code></pre>"},{"location":"sections/usage/#managing-scrapper-cache","title":"Managing Scrapper Cache","text":"<p>Over time, the Scrapper cache will grow in size, especially if you are making frequent requests with screenshots. The scrapper\u2019s cache is stored in the <code>user_data/_res</code> directory, or the configured S3-compatible bucket. You will need to set up automatic clearing of these files yourself.</p> <p>For example, you could add the following task to your cron jobs:</p> <pre><code>find /path/to/user_data/_res -ctime +7 -delete\n</code></pre> <p>This command will use the <code>find</code> utility to locate all files in the cache that were created more than 7 days ago. All such files will be deleted because the <code>find</code> utility accepts the <code>-delete</code> option.</p> <p>This is just an example of how you might deal with the scrapper\u2019s cache growing over time. You can come up with other strategies for this and implement them yourself. The main thing to remember is where Scrapper stores its cache data - by default it\u2019s in the <code>user_data/_res</code> directory.</p>"},{"location":"sections/usage/#using-scrapper","title":"Using Scrapper","text":"<p>Once the directories have been created and write permissions have been set, you can run Scrapper using the following command:</p> <pre><code>docker run -d -p 3000:3000 -v $(pwd)/user_data:/home/user/user_data -v $(pwd)/user_scripts:/home/user/user_scripts --name scrapper amerkurev/scrapper:latest\n</code></pre> <p>The Scrapper web interface should now be available at http://localhost:3000/. Use any modern browser to access it.</p> <p>To connect to Scrapper logs, use the following command:</p> <pre><code>docker logs -f scrapper\n</code></pre>"},{"location":"sections/usage/#configuration","title":"Configuration","text":"<p>Scrapper can be configured using environment variables. Here are the available options:</p> Variable Description Default <code>USER_DATA_DIR</code> Directory for storing browser session data and cache <code>./user_data</code> <code>USER_SCRIPTS_DIR</code> Directory containing custom JavaScript scripts <code>./user_scripts</code> <code>BROWSER_CONTEXT_LIMIT</code> Maximum number of concurrent browser contexts <code>20</code> <code>SCREENSHOT_TYPE</code> Image format for screenshots (<code>jpeg</code> or <code>png</code>) <code>jpeg</code> <code>SCREENSHOT_QUALITY</code> Image quality for screenshots (0-100) <code>80</code> <code>CACHE_TYPE</code> Storage type for caching results (<code>filesystem</code> or <code>s3</code>) <code>filesystem</code> <code>LOGLEVEL</code> Logging level (<code>debug</code>, <code>info</code>, <code>warning</code>, <code>error</code>, <code>critical</code>) <code>info</code>"},{"location":"sections/usage/#s3-cache-configuration","title":"S3 Cache Configuration","text":"<p>When using <code>CACHE_TYPE=s3</code>, the following S3 settings are required:</p> Variable Description Default <code>S3_BUCKET</code> S3 bucket name for storing cache <code>S3_ACCESS_KEY</code> S3 access key <code>S3_SECRET_KEY</code> S3 secret key <code>S3_ENDPOINT_URL</code> Optional endpoint URL for S3-compatible services <p>These can be set in a <code>.env</code> file for docker compose, or passed to docker run using the <code>--env-file .env</code>.</p>"}]}